---
title: "concepts"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(childesr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(tidyverse)
library(text2vec)
library(ggcorrplot)
library(factoextra)
library(cluster)

`%!in%` = Negate(`%in%`)
```

Get all utterances from childes where language is English and age of target child is less than 3.
```{r}
all_utterances <- get_utterances(language = "eng")
all_utterances_5 <- dplyr::filter(read.csv("all_utterances_6.csv"), target_child_age < 60)
all_utterances_4 <- dplyr::filter(all_utterances_5, target_child_age < 48)
```

```{r}
write.csv(all_utterances_4, 'all_utterances_4.csv')
```

//Other R script to get basic terms and superordinate categories from wordbank.
//Python script to generate a list of co-occurrences of basic terms and superordinate categories in utterances from childes.//

Group responses by age range and count occurrences of basic term within superordinate categories. Then join together all dataframes for particular window sizes. 
```{r}
df_0 <- `concept_and_instance_all_verbs_n=0` %>%
    mutate(age = cut(V6, c(0, 36))) %>% 
    mutate(wndw = 0) %>%
    group_by(wndw, V1, V5, age) %>%
    summarise(count=n())
df_1 <- `concept_and_instance_all_verbs_n=1` %>%
    mutate(age = cut(V6, c(0,36))) %>% 
    mutate(wndw = 1) %>%
    group_by(wndw, V1, V5, age) %>%
    summarise(count=n())
df_2 <- `concept_and_instance_all_verbs_n=2` %>%
    mutate(age = cut(V6, c(0,36))) %>% 
    mutate(wndw = 2) %>%
    group_by(wndw, V1, V5, age) %>%
    summarise(count=n())
df_3 <- `concept_and_instance_all_verbs_n=3` %>%
    mutate(age = cut(V6, c(0, 36))) %>% 
    mutate(wndw = 3) %>%
    group_by(wndw, V1, V5, age) %>%
    summarise(count=n())
dfMerge <- do.call("rbind", list(df_0))
```

Plot the number of co-occurrences of basic terms and superordinate categories as age increases according to superordinate category and window size.
```{r}
occurrence_change <- ggplot(dfMerge, aes(age, count, colour=V5)) + 
    geom_line(aes(group = V5)) + geom_point(aes(group = V5)) + 
    facet_grid(rows = vars(wndw), vars(V1))
```

Calculate weightings of each superordinate category for each basic term (number of co-occurrences * 1/window size+1), then sum weightings across all window sizes for equivalent basic terms. 
```{r}
dfMerge <- mutate(dfMerge, weight = count * (1/(wndw + 1)))
dfSum <- dfMerge %>% group_by(V1, V5) %>%
  summarize(weight = sum(weight))
df <- spread(dfSum, V1, weight, fill=0)
df <- df %>% remove_rownames %>% column_to_rownames(var="V5")
```

Calculate word-word similarity using cosine similarity for all word pairs.
```{r}
cos_sim <- sim2(as.matrix(df), as.matrix(df), method="cosine")
```

Plot heatmap of word-word similarities, and save plot as png.
```{r}
heatmap <- ggcorrplot(cosSim_cds, hc.order = TRUE) + theme(axis.text.x = element_text(size=9), axis.text.y = element_text(size=9))
png("heatmap_cosSim_wndw=10.png", width = 1600, height = 1600)
plot(heatmap)
dev.off()
```

Clean up corpus data
```{r}
library(udpipe)

all_utterances_3 <- mutate(all_utterances_3, lemma = gloss)

for (row in 1:nrow(all_utterances_3)){
  x <- udpipe(x = toString(all_utterances_3[row, 7]), object = "english")
  lemmatize <- x$lemma
  all_utterances_3$lemma[row] <- lemmatize
}
 #right now still only taking the first word.
#This takes forever and is bad
```

Train word2vec model, calculate cosine similarities of nouns from wordbank
```{r}
library(wordVectors)
model = train_word2vec("all_utterances_3.txt",
                            output="all_utterances_3_w2v.bin", threads = 4,
                            vectors = 100, window=10, cbow=1, min_count = 10, force= TRUE)

model_cds = read.vectors("all_utterances_3_w2v.bin")

#Need to get data (unique_items) from wordbank here

cue_words <- c(unique_items$food_drink, unique_items$animals, unique_items$clothing, unique_items$furniture_rooms, unique_items$toys)

model_cue_cds <- model_cds[[which(rownames(model_cds) %in% cue_words), average=FALSE]]
#model_target_cds <- model_cds[[which(rownames(model_cds) %in% target_words), average=FALSE]]
cosSim_cds <- cosineSimilarity(model_cue_cds, model_cue_cds)

```

Compute hierarchical clustering
```{r}
# Dissimilarity matrix
d <- dist(cos_sim, method = "euclidean")
# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "average" )
# Plot the obtained dendrogram--do we need any other kinds of graphs???
plot(hc1, cex=0.6, hang = -1)
```

Compute optimal number of clusters
```{r}
set.seed(123)
num <- fviz_nbclust(cosSim_cds, hcut,  method = "silhouette")+
  labs(subtitle = "Silhouette statistic method")
fviz_nbclust(cosSim_cds, hcut,  method = "gap_stat", nboot = 50)+
  labs(subtitle = "Gap statistic method")
fviz_nbclust(cosSim_cds, hcut,  method = "silhouette")+
  labs(subtitle = "Silhouette statistic method")
fviz_nbclust(cosSim_cds, hcut,  method = "wss")+
  labs(subtitle = "WSS statistic method")
print(num)
```

Looking at plots
```{r}
clust <- cutree(hc1, k = 3)
fviz_cluster(list(data = cosSim_Cds, cluster = clust)) 
```

Compute optimal number of clusters
Uses silhouette method, perhaps not the best measure? (but easiest to implement)
```{r}
#This could theoretically do everything for clustering, so maybe simplify code in the future


library(NbClust)
optimal_nbcluster <- function(sim_data, dis){
  vals <- c(0, 0, 0)
  
  vals[1] <- NbClust(data = sim_data, diss = dis, distance = NULL, min.nc = 1, max.nc = 10, method = "average", index = "kl")$Best.nc[1]
  
  #vals[2] <- NbClust(data = sim_data, diss = dis, distance = NULL, min.nc = 1, max.nc = 10, method = "average", index = "gap")$Best.nc[1]
  
  #vals[3] <- NbClust(data = sim_data, diss = dis, distance = NULL, min.nc = 1, max.nc = 10, method = "average", index = "silhouette")$Best.nc[1]
  
  optimal_clust_size <- max(vals)
  return(max(vals))
}
```

Compute explicit clustering
```{r}
explicit_clustering <- function(filename, age){
  
  df_0 <- read.csv(filename, header = FALSE)  %>%
    #should maybe change this line
    mutate(ageYr = cut(V6, c(0, age * 12 + 24))) %>% 
    mutate(wndw = 0) %>%
    group_by(wndw, V1, V5, ageYr) %>%
    summarise(count=n())
  
  df_0 <- df_0[!is.na(df_0$ageYr), ]
  
  dfMerge <- do.call("rbind", list(df_0))
  dfMerge <- mutate(dfMerge, weight = count * (1/(wndw + 1)))
  dfSum <- dfMerge %>% group_by(V1, V5) %>%
    summarise(weight = sum(weight))
  df <- spread(dfSum, V1, weight, fill=0)
  df <- df %>% remove_rownames %>% column_to_rownames(var="V5")
  cos_sim <- sim2(as.matrix(df), as.matrix(df), method="cosine")
  saveRDS(cos_sim, paste(toString(age), "cos_sim_noVerbs.rds", sep="_"))

  # Dissimilarity matrix
  d <- dist(cos_sim, method = "euclidean")
  # Hierarchical clustering using Complete Linkage
  hc1 <- hclust(d, method = "average" )
  clust <- cutree(hc1, k = optimal_nbcluster(cos_sim, d))
  clust[order(as.numeric(names(clust)))]
  clust_file <- paste(toString(age), "explicit.rds", sep = "_")
  saveRDS(clust, clust_file )
  write(sapply(names(clust),function(x) paste(x,paste(clust[[x]],collapse=" "))), file = 'clusters.txt', append=TRUE)
  return(clust)
}
```

Compute w2v clustering
```{r}
library(wordVectors)
w2v_clustering <- function(filename, age){
  model = train_word2vec(filename,
                         output="all_utterances_w2v.bin", threads = 4,
                         vectors = 100, window=10, cbow=1, min_count = 10, force= TRUE)
  
  model_cds = read.vectors("all_utterances_w2v.bin")
  
  cue_words <- c(unique_items$food_drink, unique_items$animals, unique_items$clothing, unique_items$furniture_rooms, unique_items$toys)
  
  model_cue_cds <- model_cds[[which(rownames(model_cds) %in% cue_words), average=FALSE]]
  #model_target_cds <- model_cds[[which(rownames(model_cds) %in% target_words), average=FALSE]]
  cosSim_cds <- cosineSimilarity(model_cue_cds, model_cue_cds)
  saveRDS(cosSim_cds, paste(toString(age), "cosSim.rds", sep="_"))
  
  # Hierarchical clustering using Complete Linkage
  # Dissimilarity matrix
  d <- dist(cosSim_cds, method = "euclidean")
  # Hierarchical clustering using Complete Linkage
  hc1 <- hclust(d, method = "average" )
  #clust <- cutree(hc1, k = optimal_nbcluster(cosSim_cds, d))
  clust <- cutree(hc1, k = 4)
  clust[order(as.numeric(names(clust)))]
  clust_file <- paste(toString(age), "w2v.rds", sep = "_")
  saveRDS(clust, clust_file )
  write(sapply(names(clust),function(x) paste(x,paste(clust[[x]],collapse=" "))), file = 'clusters.txt', append=TRUE)
  return(clust)
}
```

Compute number of matches for categories
```{r}
correct_pairings <- function(category, clustering){
  #Compute number of pairings that occur in the same category
  countCtg <- vector(mode = "numeric", length = max(clustering))
  for (instance in clustering[which(names(clustering) %in% category)]){
    countCtg[instance] = countCtg[instance] + 1
  }
  pairs <- 0
  for (num in countCtg){
    pairs = pairs + choose(n = num, 2)
  }
  #print(category)
  #print(countCtg)
  #print(table(clustering))
  
  #Compute number of possible correct pairings for a category
  total_correct_pairs <- choose(n = sum(countCtg), k = 2)
  
  return(correct_pairs <- pairs/total_correct_pairs)
}
```

Compute number of possible wrong pairings
```{r}
wrong_pairings <- function(category, clustering) {
  #Compute number of wrong pairings
  pairs <- 0
  size <- 0
  for (num in min(clustering):max(clustering)){
    count_wrong <- 0
    clust_size <- 0
    index <- 0
    for (instance in clustering){
      index = index + 1
      if (instance == num){
        clust_size = clust_size + 1
        if (names(clustering[index]) %!in% category){
          count_wrong = count_wrong + 1
        }
        else{
          size = size + 1
        }
      }
    }
    pairs = pairs + ((clust_size - count_wrong) * count_wrong)
  }
  #Compute number of possible different pairings
  total_wrong_pairs <- (length(clustering) - size) * size
  
  return(wrong_pairs <- pairs/total_wrong_pairs)
}
```

Compute correct/wrong pairings for word2vec and explicit cues for ages 3-6
Takes approx. 20 min to run when computing clusters
```{r}
unique_items <- readRDS("unique_items.rds")

cluster_df <- data.frame(age = c(), category_name = c(), model = c(), correct = c(), score = c())
txt_files <- c("all_utterances_3.txt", "all_utterances_4.txt", "all_utterances_5.txt", "all_utterances_6.txt")
#concept_files <- c("concept_and_instance_all_verbs_n=0_3.csv", "concept_and_instance_all_verbs_n=0_4.csv", "concept_and_instance_all_verbs_n=0_5.csv", "concept_and_instance_all_verbs_n=0_6.csv")

concept_files <- c("concept_and_instance_all_n=0.csv", "concept_and_instance_all_n=0.csv", "concept_and_instance_all_n=0.csv", "concept_and_instance_all_n=0.csv")

cue_words <- unique_items[c("toys", "food_drink", "animals", "clothing", "furniture_rooms")]
write("", file = "clusters.txt", append = FALSE)
#optimal_clust_size <- 0

for (age in 1:length(txt_files)){
  title <- paste("W2V", toString(age + 2), sep = " ")
  write(title, file = 'clusters.txt', append = TRUE)
  #w2vClust <- w2v_clustering(filename = txt_files[age], age)
  w2v_file <- paste(toString(age), "w2v.rds", sep="_")
  w2vClust <- readRDS(w2v_file)
  #randomClust_w2v <- random_clustering(max(w2vClust), names(w2vClust))
  
  title <- paste("EXPLICIT", toString(age + 2), sep = " ")
  write(title, file = 'clusters.txt', append = TRUE)
  explicitClust <- explicit_clustering(filename = concept_files[age], age)
  #explicit_file <- paste(toString(age), "explicit.rds", sep="_")
  #explicitClust <- readRDS(explicit_file)
  #randomClust_explicit <- random_clustering(max(explicitClust), names(explicitClust))
  
  for (cat in names(cue_words)){
    temp = data.frame(age = age+2, category_name = c(cat), model = c("w2v"), correct = c("correct"), score = c(correct_pairings(category = cue_words[[cat]], clustering = w2vClust)))
    cluster_df <- rbind(cluster_df, temp)
    temp = data.frame(age = age+2, category_name = c(cat), model = c("w2v"), correct = c("wrong"), score = c(wrong_pairings(category = cue_words[[cat]], clustering = w2vClust)))
    cluster_df <- rbind(cluster_df, temp)
    temp = data.frame(age = age+2, category_name = c(cat), model = c("explicit"), correct = c("correct"), score = c(correct_pairings(category = cue_words[[cat]], clustering = explicitClust)))
    cluster_df <- rbind(cluster_df, temp)
    temp = data.frame(age = age+2, category_name = c(cat), model = c("explicit"), correct = c("wrong"), score = c(wrong_pairings(category = cue_words[[cat]], clustering = explicitClust)))
    cluster_df <- rbind(cluster_df, temp)
    
    #Calculate random clusters for comparison
    # temp = data.frame(age = age+2, category_name = c(cat), model = c("random_w2v"), correct = c("correct"), score = c(correct_pairings(category = cue_words[[cat]], clustering = randomClust_w2v)))
    # cluster_df <- rbind(cluster_df, temp)
    # temp = data.frame(age = age+2, category_name = c(cat), model = c("random_w2v"), correct = c("wrong"), score = c(wrong_pairings(category = cue_words[[cat]], clustering = randomClust_w2v)))
    # cluster_df <- rbind(cluster_df, temp)
    # temp = data.frame(age = age+2, category_name = c(cat), model = c("random_explicit"), correct = c("correct"), score = c(correct_pairings(category = cue_words[[cat]], clustering = randomClust_explicit)))
    # cluster_df <- rbind(cluster_df, temp)
    # temp = data.frame(age = age+2, category_name = c(cat), model = c("random_explicit"), correct = c("wrong"), score = c(wrong_pairings(category = cue_words[[cat]], clustering = randomClust_explicit)))
    # cluster_df <- rbind(cluster_df, temp)
  }
}

write.csv(cluster_df, "cluster_df2.csv")
```

Plot goodness of clusters
```{r}
library(ggplot2)
p <- ggplot(cluster_df, aes(age, score, colour=category_name)) + 
    geom_line() + 
    geom_point() + 
    facet_grid(vars(correct), vars(model))
p
```

Compute randomization for comparison
```{r}
#Randomly assign each member of category to cluster
#Compute same correct/incorrect scorings
#Use same number of clusters as w2v/explicit respectively
random_clustering <- function(num, words){
  clust <- rep(0, length(words))
  for (val in 1:length(clust)){
    clust[val] = sample(1:num, 1)
  }
  names(clust) = words
  return(clust)
}
```

Compute F measure of clusters
```{r}
library(FlowSOM)
f_measure <- function(){
  
}
```

TO DO:
-see how different clusertings effect measurements to get understanding of extreme cases
  especially figure out what happens if you vary the size of k
  keep denominator constant
  use cases of k that we already have and numbers of clusters that we have (keep in range of interest)
  things we care about:
    different sizes of k with the same cluster size
    probably other things
  is there a way to modify the correct function so that it can give us better intuition about the clustering?
  incorrect measure is probably more stable
-recompute number optimal clusterings each time to run (DONE)
  
-recompute clusters by hand to see how they work -> see if measurements make sense
-normalize across categories to standardize size
  #of non-x in cluster y/# of not x-> should be the same as calculating pairings for incorrect
  incorrect is normalized across size
  actually... idk think more about the formula and if its right for all cases
-add output of categorizations to slack
-add cluster sizes to the graph/some output?

-random controls:
  randomize once, then do same analysis
  do a randomization for each clustering size
  do like 10 randomizations and see how the analyses vary

```{r}
x <- c(25)
y <- c(50)
print(sum(x * (y - x))/(25 * (150 - 25)))
x <- c(30, 30)
y <- c(60, 60)
print(sum(x * (y - x))/(60 * (150 - 60)))
choose(10, 2) * 4/choose(40, 2)
choose(1, 2)*40/choose(40, 2)

#print(sum(x)/choose(50, 2))
```

Find clustering coefficient?
```{r}
library(graph)
library(rlist)

clust <- readRDS("4_w2v.rds")
myList <- list()
for (x in 1:max(clust)){
  myList[[length(myList)+1]] <- clust[clust==x]
}
cG1 <- new("clusterGraph", clusters=myList)

```

Density plots
```{r}
library(reshape2)
library(ggplot2)

#filenames <- c("1_cos_sim.rds", "2_cos_sim.rds", "3_cos_sim.rds", "4_cos_sim.rds")
#filenames <- c("1_cosSim.rds", "2_cosSim.rds", "3_cosSim.rds", "4_cosSim.rds")
filenames <- c("1_cos_sim_noVerbs.rds", "2_cos_sim_noVerbs.rds", "3_cos_sim_noVerbs.rds", "4_cos_sim_noVerbs.rds")

density_df <- data.frame()

for (file in filenames){
  
  sim <- readRDS(file)
  sim_long <- melt(sim)
  cat1 <- rep(NA, nrow(sim_long))
  cat2 <- rep(NA, nrow(sim_long))
  age <- rep(which(filenames == file) + 2, nrow(sim_long))
  measure <- rep(NA, nrow(sim_long))
  measure <- rep(NA, nrow(sim_long))
  sim_long <- cbind(sim_long, cat1)
  sim_long <- cbind(sim_long, cat2)
  sim_long <- cbind(sim_long, measure)
  sim_long <- cbind(sim_long, age)
  
  for (row in 1:nrow(sim_long)){
    sim_long$cat1[row] <- names(cue_words)[sapply(seq_along(cue_words),function(x){sim_long$Var1[row] %in% cue_words[[x]]})]
    sim_long$cat2[row] <- names(cue_words)[sapply(seq_along(cue_words),function(x){sim_long$Var2[row] %in% cue_words[[x]]})]
    
    #Trying to be clever here....
    #sim_long$cat1["Var1" == sim_long$Var1[row]] <- sim_long$cat1[row]
    #sim_long$cat2["Var2" == sim_long$Var2[row]] <- sim_long$cat2[row]
  }
  sim_long[sim_long$cat1 == sim_long$cat2, ]$measure <- "within"
  sim_long[sim_long$cat1 != sim_long$cat2, ]$measure <- "between"
  density_df <- rbind(density_df, sim_long)
}

#ggplot(density_df[density_df$cat1 == "food_drink", ], aes(value, fill = measure)) + geom_density(alpha = 0.2) + facet_wrap(vars(age, cat1), scales="free_y") +ggtitle("Explicit Cohesiveness (food_drink)")

#food_df <- density_df[density_df$cat1 == "food_drink", ]

ggplot(density_df, aes(value, fill = measure)) + geom_density(alpha = 0.2) + facet_wrap(vars(age, cat1), scales="free_y") +ggtitle("Explicit w/o Verbs Cohesiveness")

```

ROCs and AUCs
```{r}
library(pROC)
auc_df <- data.frame(animals=numeric(4), food_drink=numeric(4), toys=numeric(4), furniture_rooms=numeric(4), clothing=numeric(4))

for (var in unique(density_df$cat1)){
  for (num in unique(density_df$age)){
    
    density_df2 <- density_df[density_df$cat1 == var & density_df$age == num, ]
    density_df2$measure[density_df2$measure == "between"] <- 0
    density_df2$measure[density_df2$measure == "within"] <- 1
    density_df2$measure <- as.numeric(density_df2$measure)
    
    auc_df[num - 2, var] <- auc(response = density_df2$measure, predictor = density_df2$value)
  }
}

rownames(auc_df) <- unique(density_df$age)
auc_df

```
modularity- average of edge weights for in-group and across-group, normalize by number of possible edges?
sum of edges/number of edges (within)
also good to have some variance
between 
sum of between group connections/sum of total between group connections
cohesiveness: w/b
also plot distribution to see if there are any outliers- some way to do this in ggplot
curvy plot, one for each category
every category has within and between for same graph (within should be higher)
use similarity matrix as "edge weights"

potential problems:
the ages are 3, 3-4, 3-5, 3-6. would it be better to look only at disjoint sets of ages?
calculating distance twice?


probability is proportional to the cosine similarity normalized for size of group
sample as long as the second object in the category is the same as the first
print count of objects
sample and then compute probability in each iteration
for each pair of categories--sample instances, sample item you start with, sample next item until you switch categories
make a categorial distribution with cosine similarities and then sample from that
maximum number of iterations is 4
how many instances we should sample to get a stable distribution (start with around 100-200)
start with only up to 3

Feature values for verbs????

Replicate experiment by sampling from categories, calculating mean run length...
```{r replicate_experiment}
library(extraDistr)

#cosSim <- readRDS("1_cos_sim_explicit.rds")
all_random <- list()

for (concept1 in names(cue_words)){
  catRun <- list()
  for (concept2 in names(cue_words[ - which(names(cue_words) == concept1)])){
    run <- as.numeric()
    
    for (i in 1:500){
    count <- 0
    
    concept1words <- cue_words[[concept1]][which(cue_words[[concept1]] %in% row.names(cosSim))]
    concept2words <- cue_words[[concept2]][which(cue_words[[concept2]] %in% row.names(cosSim))]
    
    sample_group <- c(sample(concept1words, 4), sample(concept2words, 4))
    
    #make a categorical distribution w/ cosine similarities as probabilities
    count <- count + 1
    first <- sample(sample_group, replace = FALSE, 1)
    continue <- TRUE
    
    while(continue){
      sample_group <- sample_group[sample_group != first]

      # temp_probability <- c()
      # for (instance in sample_group){
      #   temp_probability <- c(temp_probability, 1 + cosSim[first, instance])
      # }
      # probability <- temp_probability/sum(temp_probability)
      
      probability <- rep(1/length(sample_group), length(sample_group))
      
      next_instance <- sample_group[rcat(1, probability)]
      
      #temp_cosSim <- select(as.data.frame(cosSim), one_of(sample_group))[first, ]
      #next_instance <- colnames(temp_cosSim)[which.max(temp_cosSim)]
      
      #check if category of first and next are the same
      if (first %in% cue_words[[concept1]] & next_instance %in% cue_words[[concept1]] | first %in% cue_words[[concept2]] & next_instance %in% cue_words[[concept2]]){
        count <- count + 1
        first <- next_instance
        if (count == 4){
          run <- c(run, count)
          count <- 0
          continue <- FALSE
        }
      }
      else{
        #first <- next_instance
        run <- c(run, count)
        count <- 0
        continue <- FALSE
      }
    }
    }
    #catRun[[concept1]] <- rep(NA, 100)
    catRun[[concept2]] <- run
    catRun <- catRun[order(names(catRun))]
  }
  all_random[[concept1]] <- catRun
}

```


Graph experiment results
```{r}
library(reshape2)
library(ggplot2)

experiment_df_w2v <- melt(all_w2v)
mean_df_w2v <- aggregate(experiment_df_w2v$value, by=list(L1 = experiment_df_w2v$L1, L2 = experiment_df_w2v$L2), FUN = mean)
  
ggplot(experiment_df_w2v, mapping=aes(value)) + geom_density(alpha = 0.2) + facet_wrap(nrow= 4, vars(L2, L1)) + ggtitle("w2v experiment (deterministic)") + geom_vline(mean_df_w2v, mapping=aes(xintercept=x), color="blue", 
                 linetype="dashed")

experiment_df_explicit <- melt(all_explicit)
mean_df_explicit <- aggregate(experiment_df_explicit$value, by=list(L1 = experiment_df_explicit$L1, L2 = experiment_df_explicit$L2), FUN = mean)
  
ggplot(experiment_df_explicit, mapping=aes(value)) + geom_density(alpha = 0.2) + facet_wrap(nrow= 4, vars(L2, L1)) + ggtitle("explicit experiment (deterministic)") + geom_vline(mean_df_explicit, mapping=aes(xintercept=x), color="blue", 
                 linetype="dashed")

experiment_df_random <- melt(all_random)
mean_df_random <- aggregate(experiment_df_random$value, by=list(L1 = experiment_df_random$L1, L2 = experiment_df_random$L2), FUN = mean)
  
ggplot(experiment_df_explicit, mapping=aes(value)) + geom_density(alpha = 0.2) + facet_wrap(nrow= 4, vars(L2, L1)) + ggtitle("explicit experiment (deterministic)") + geom_vline(mean_df_explicit, mapping=aes(xintercept=x), color="blue", 
                 linetype="dashed")

experiment_df <- data.frame(L1 = experiment_df_explicit$L1, L2 = experiment_df_explicit$L2, explicit = experiment_df_explicit$value, w2v = experiment_df_w2v$value, random = experiment_df_random$value) %>% 
  melt()

experiment_df_mean <- data.frame(L1 = mean_df_explicit$L1, L2 = mean_df_explicit$L2, explicit = mean_df_explicit$x, w2v = mean_df_w2v$x, random = mean_df_random$x) %>% 
  melt()


ggplot(experiment_df_explicit, mapping=aes(value)) + geom_vline(mean_df_explicit, mapping=aes(xintercept=x), color="blue", linetype="dashed") + geom_vline(mean_df_w2v, mapping=aes(xintercept=x), color="red", linetype="dashed") + geom_vline(mean_df_random, mapping=aes(xintercept=x), color="gray", linetype="dashed") + facet_wrap(nrow= 4, vars(L2, L1)) + ggthemes::theme_few() + theme(legend.position="right")

ggplot(experiment_df_mean) + geom_vline(aes(xintercept=value, colour=variable)) + scale_colour_manual(values=c("red","blue","black")) + theme(legend.position="right", panel.background = element_rect(fill = "white", color="gray")) + facet_wrap(nrow= 4, vars(L2, L1))

```

DATA FIXES:
SHOULD COUNT VERBS AS CATEGORIZATION EVEN IF ITS INCORRECT



